# -*- coding: utf-8 -*-
"""regression_dl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11RMnTTRiLgQx6bxI2BVQRgr0oP119BIk
"""

from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error 
from matplotlib import pyplot as plt
import seaborn as sb
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings 
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=DeprecationWarning)
from xgboost import XGBRegressor
from keras.layers import Dropout
from keras import regularizers

from google.colab import drive
drive.mount('/content/gdrive')

train_copy = pd.read_csv('gdrive/My Drive/hp_train.csv')
test_copy = pd.read_csv('gdrive/My Drive/hp_test.csv')

target = train_copy.SalePrice
df = train_copy.append(test_copy)

df.drop(['SalePrice'],axis = 1 , inplace = True)

df.describe()

df.shape

nan_col = df.columns[df.isna().any()].tolist()

cols = list(set(df.columns.tolist()) - set(nan_col))

len(cols)

df_new = df[cols]

df_new.hist(figsize = (12,10))
plt.show()

train_data = train_copy[cols]
train_data['Target'] = target

C_mat = train_data.corr()
fig = plt.figure(figsize = (15,15))

sb.heatmap(C_mat, vmax = .8, square = True)
plt.show()

num_cols = df._get_numeric_data().columns

cat_cols = list(set(cols) - set(num_cols))

def oneHotEncode(df,colNames):
    for col in colNames:
        if( df[col].dtype == np.dtype('object')):
            dummies = pd.get_dummies(df[col],prefix=col)
            df = pd.concat([df,dummies],axis=1)

            #drop the encoded column
            df.drop([col],axis = 1 , inplace=True)
    return df
    

print('There were {} columns before encoding categorical features'.format(df_new.shape[1]))
df_new = oneHotEncode(df_new, cat_cols)
print('There are {} columns after encoding categorical features'.format(df_new.shape[1]))

def split_combined(df_new):
    #global combined
    train = df_new[:1460]
    test = df_new[1460:]

    return train , test 
  
train, test = split_combined(df_new)

from sklearn import preprocessing

min_max_scaler = preprocessing.MinMaxScaler()
train = min_max_scaler.fit_transform(train)

NN_model = Sequential()

# The Input Layer :
NN_model.add(Dense(256, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))

# The Hidden Layers :
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu', kernel_regularizer=regularizers.l2(0.03)))
Dropout(0.3),
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu', kernel_regularizer=regularizers.l2(0.03)))
Dropout(0.3),
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu', kernel_regularizer=regularizers.l2(0.03)))
Dropout(0.3),

# The Output Layer :
NN_model.add(Dense(1, kernel_initializer='normal',activation='linear', kernel_regularizer=regularizers.l2(0.03)))

# Compile the network :
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_model.summary()

checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' 
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

hist = NN_model.fit(train, target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)

import matplotlib.pyplot as plt
#% matplotlib qt 


plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.show()

plt.plot(hist.history['mean_absolute_error'])
plt.plot(hist.history['val_mean_absolute_error'])
plt.title('Model Error')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()

wights_file = 'Weights-398--19476.21260.hdf5' # choose the best checkpoint 
NN_model.load_weights(wights_file) # load it
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])

predictions = NN_model.predict(test)

#loss, acc = NN_model.evaluate(X_test, Y_test)

train_X, val_X, train_y, val_y = train_test_split(train, target, test_size = 0.25, random_state = 14)

model = RandomForestRegressor()
model.fit(train_X,train_y)

# Get the mean absolute error on the validation data
predicted_prices = model.predict(val_X)
MAE = mean_absolute_error(val_y , predicted_prices)
print('Random forest validation MAE = ', MAE)

XGBModel = XGBRegressor()
XGBModel.fit(train_X,train_y , verbose=False)

# Get the mean absolute error on the validation data :
XGBpredictions = XGBModel.predict(val_X)
MAE = mean_absolute_error(val_y , XGBpredictions)
print('XGBoost validation MAE = ',MAE)

